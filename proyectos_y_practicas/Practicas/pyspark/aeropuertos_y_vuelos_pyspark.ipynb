{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ba5f9b0-867d-442c-80fa-3dffdc199a9a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#En la carpeta \"data_cp_flights\" hay un archivo Parquet, comprimido con Snappy, en adelante, la tabla \"flights\". ¿Qué cantidad de columnas posee?\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"consulta-parquet\").getOrCreate()\n",
    "parquet_path = \"dbfs:/FileStore/shared_uploads/alejodiezgomez@gmail.com/raw_flight_data_snappy-1.parquet\"\n",
    "df = spark.read.parquet(parquet_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e7fb90d-8f81-4dfa-a6f0-b703a8912879",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre del DataFrame: DataFrame[DayofMonth: int, DayOfWeek: int, Carrier: string, OriginAirportID: int, DestAirportID: int, DepDelay: int, ArrDelay: int]\n+----------+---------+-------+---------------+-------------+--------+--------+\n|DayofMonth|DayOfWeek|Carrier|OriginAirportID|DestAirportID|DepDelay|ArrDelay|\n+----------+---------+-------+---------------+-------------+--------+--------+\n|        30|        4|     UA|          13930|        10721|      -3|      -7|\n|        30|        4|     UA|          11618|        12892|      -1|     -28|\n|        30|        4|     UA|          12892|        14771|      89|      84|\n|        30|        4|     UA|          13930|        15304|      -2|      -2|\n|        30|        4|     UA|          11618|        12266|       7|      23|\n|        30|        4|     UA|          12266|        13851|      -2|      -7|\n|        30|        4|     UA|          13930|        12953|      72|      65|\n|        30|        4|     UA|          14771|        11618|     164|     136|\n|        30|        4|     UA|          12266|        13930|      10|      39|\n|        30|        4|     UA|          12892|        12266|       0|      -4|\n|        30|        4|     UA|          13930|        11042|      68|      64|\n|        30|        4|     UA|          10721|        11292|       3|      14|\n|        30|        4|     UA|          12892|        11292|       8|      -1|\n|        30|        4|     UA|          12266|        12339|      -1|      18|\n|        30|        4|     UA|          14057|        12266|     -10|     -43|\n|        30|        4|     UA|          14747|        12264|      60|      66|\n|        30|        4|     UA|          11618|        13930|      -7|     -43|\n|        30|        4|     UA|          11697|        11618|      -2|      -7|\n|        30|        4|     UA|          13930|        10299|     141|     122|\n|        30|        4|     UA|          12266|        13930|      86|      96|\n+----------+---------+-------+---------------+-------------+--------+--------+\nonly showing top 20 rows\n\nAeropuertos con mayor cantidad de vuelos:\nOriginAirportID: 14771\nDestAirportID: 12892\nCantidad de vuelos: 9367\n"
     ]
    }
   ],
   "source": [
    "#Quitar de la tabla \"flights\" los registros que tengan valores nulos o faltantes en los campos \"ArrDelay\" y \"DepDelay\". Luego contestar las siguientes preguntas:\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Crea una sesión de Spark\n",
    "spark = SparkSession.builder.appName(\"consulta-parquet\").getOrCreate()\n",
    "\n",
    "# Ruta al archivo Parquet\n",
    "parquet_path = \"dbfs:/FileStore/shared_uploads/alejodiezgomez@gmail.com/raw_flight_data_snappy-1.parquet\"\n",
    "\n",
    "# Lee el archivo Parquet y crea un DataFrame\n",
    "df = spark.read.parquet(parquet_path)\n",
    "\n",
    "# Asigna un nombre temporal al DataFrame\n",
    "df.createOrReplaceTempView(\"mi_df_temporal\")\n",
    "\n",
    "# Muestra el nombre del DataFrame\n",
    "print(\"Nombre del DataFrame:\", spark.sql(\"SELECT * FROM mi_df_temporal LIMIT 1\"))\n",
    "\n",
    "# Filtra los registros que no tienen valores nulos en las columnas especificadas\n",
    "mi_tabla_sin_nulos = spark.sql(\"SELECT * FROM mi_df_temporal WHERE ArrDelay IS NOT NULL AND DepDelay IS NOT NULL\")\n",
    "\n",
    "# Muestra el DataFrame resultante\n",
    "mi_tabla_sin_nulos.show()\n",
    "#-----------------------------------------------------------------------------------\n",
    "#¿Cuál es la tupla de aeropuertos, con mayor cantidad de vuelos entre sí? Nota: Es posible tomar el nombre del aeropuerto desde el archivo \"airports.json\" ubicado en la carpeta \"data_cp_airports\", donde \"airport_id\" se puede #relacionar con \"OriginAirportID\" y \"DestAirportID\" de la tabla \"flights\" \n",
    "\n",
    "\n",
    "# Agrupa por pares de aeropuertos y cuenta la cantidad de vuelos\n",
    "pairs_count = mi_tabla_sin_nulos.groupBy(\"OriginAirportID\", \"DestAirportID\").count()\n",
    "\n",
    "# Encuentra el par con la mayor cantidad de vuelos\n",
    "max_flight_pair = pairs_count.orderBy(col(\"count\").desc()).first()\n",
    "\n",
    "# Imprime la información del par con la mayor cantidad de vuelos\n",
    "print(\"Aeropuertos con mayor cantidad de vuelos:\")\n",
    "print(\"OriginAirportID:\", max_flight_pair[\"OriginAirportID\"])\n",
    "print(\"DestAirportID:\", max_flight_pair[\"DestAirportID\"])\n",
    "print(\"Cantidad de vuelos:\", max_flight_pair[\"count\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dae50d13-a83a-4054-bda3-0eaca36e24b0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-----+--------------------+\n|airport_id|    city|state|                name|\n+----------+--------+-----+--------------------+\n|     12173|Honolulu|   HI|Honolulu Internat...|\n+----------+--------+-----+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "#consultar nombres ( Es posible tomar el nombre del aeropuerto desde el archivo \"airports.csv\", donde \"airport_id\" se puede relacionar con \"OriginAirportID\" y \"DestAirportID\" de la tabla \"flights\")\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Crea una sesión de Spark\n",
    "spark = SparkSession.builder.appName(\"consulta-sql\").getOrCreate()\n",
    "\n",
    "# Lee el archivo CSV y crea un DataFrame llamado df1\n",
    "df1 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"dbfs:/FileStore/shared_uploads/alejodiezgomez@gmail.com/airports-1.csv\")\n",
    "\n",
    "# Crea una vista temporal con el nombre \"mi_df_temporal2\" para el DataFrame df1\n",
    "df1.createOrReplaceTempView(\"mi_df_temporal2\")\n",
    "\n",
    "# Número que quieres buscar en la columna \"airport_id\"\n",
    "numero_ingresado = 12173\n",
    "\n",
    "# Ejecuta la consulta SQL para seleccionar todas las columnas donde \"airport_id\" es igual al número ingresado\n",
    "resultado = spark.sql(f\"SELECT * FROM mi_df_temporal2 WHERE airport_id = {numero_ingresado}\")\n",
    "\n",
    "# Muestra el resultado\n",
    "resultado.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71253426-163c-4ed5-9fe3-c8fa7348351a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aeropuertos con mayor cantidad de vuelos demorados:\nOriginAirportID: 12892\nDestAirportID: 14771\nCantidad de vuelos demorados: 2375\n"
     ]
    }
   ],
   "source": [
    "#Si consideramos que cuando el campo ArrDelay es mayor a 25, el vuelo está demorado, contestar: ¿Cuál es la tupla de aeropuertos, con mayor cantidad de vuelos demorados entre sí?\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Crea una sesión de Spark\n",
    "spark = SparkSession.builder.appName(\"consulta-parquet\").getOrCreate()\n",
    "\n",
    "# Ruta al archivo Parquet\n",
    "parquet_path = \"dbfs:/FileStore/shared_uploads/alejodiezgomez@gmail.com/raw_flight_data_snappy-1.parquet\"\n",
    "\n",
    "# Lee el archivo Parquet y crea un DataFrame\n",
    "df = spark.read.parquet(parquet_path)\n",
    "\n",
    "# Asigna un nombre temporal al DataFrame\n",
    "df.createOrReplaceTempView(\"mi_df_temporal\")\n",
    "\n",
    "# Filtra los vuelos demorados (ArrDelay > 25)\n",
    "vuelos_demorados = spark.sql(\"SELECT * FROM mi_df_temporal WHERE ArrDelay > 25\")\n",
    "\n",
    "# Agrupa por pares de aeropuertos y cuenta la cantidad de vuelos demorados\n",
    "pares_demorados_count = vuelos_demorados.groupBy(\"OriginAirportID\", \"DestAirportID\").count()\n",
    "\n",
    "# Encuentra el par con la mayor cantidad de vuelos demorados\n",
    "max_demorados_pair = pares_demorados_count.orderBy(col(\"count\").desc()).first()\n",
    "\n",
    "# Imprime la información del par con la mayor cantidad de vuelos demorados\n",
    "print(\"Aeropuertos con mayor cantidad de vuelos demorados:\")\n",
    "print(\"OriginAirportID:\", max_demorados_pair[\"OriginAirportID\"])\n",
    "print(\"DestAirportID:\", max_demorados_pair[\"DestAirportID\"])\n",
    "print(\"Cantidad de vuelos demorados:\", max_demorados_pair[\"count\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b200b9a-2ec7-454c-86b4-47e01b0b8312",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aeropuerto con el mayor promedio de demora en arribos:\nOriginAirportID: 12173\nPromedio de demora en arribos: 94.41454272863568\n"
     ]
    }
   ],
   "source": [
    "#Si consideramos que cuando el campo ArrDelay es mayor a 25, el vuelo está demorado, contestar: ¿Cuál es el aeropuerto con el mayor promedio de demora en arribos, teniendo en cuenta el de orígen?\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Crea una sesión de Spark\n",
    "spark = SparkSession.builder.appName(\"consulta-parquet\").getOrCreate()\n",
    "\n",
    "# Ruta al archivo Parquet\n",
    "parquet_path = \"dbfs:/FileStore/shared_uploads/alejodiezgomez@gmail.com/raw_flight_data_snappy-1.parquet\"\n",
    "\n",
    "# Lee el archivo Parquet y crea un DataFrame\n",
    "df = spark.read.parquet(parquet_path)\n",
    "\n",
    "# Filtra los vuelos demorados (ArrDelay > 25)\n",
    "vuelos_demorados = df.filter(col(\"ArrDelay\") > 25)\n",
    "\n",
    "# Agrupa por aeropuerto de origen y calcula el promedio de demora en arribos\n",
    "promedio_demora_por_aeropuerto = vuelos_demorados.groupBy(\"OriginAirportID\").agg({\"ArrDelay\": \"avg\"})\n",
    "\n",
    "# Encuentra el aeropuerto con el mayor promedio de demora en arribos\n",
    "max_promedio_aeropuerto = promedio_demora_por_aeropuerto.orderBy(col(\"avg(ArrDelay)\").desc()).first()\n",
    "\n",
    "# Imprime la información del aeropuerto con el mayor promedio de demora en arribos\n",
    "print(\"Aeropuerto con el mayor promedio de demora en arribos:\")\n",
    "print(\"OriginAirportID:\", max_promedio_aeropuerto[\"OriginAirportID\"])\n",
    "print(\"Promedio de demora en arribos:\", max_promedio_aeropuerto[\"avg(ArrDelay)\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9abb319-6d6e-4000-9395-8014260faac7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de vuelos demorados: 380502\n"
     ]
    }
   ],
   "source": [
    "#Si consideramos que cuando el campo ArrDelay es mayor a 25, el vuelo está demorado, contestar: ¿Cuál es la cantidad de vuelos demorados?\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Crea una sesión de Spark\n",
    "spark = SparkSession.builder.appName(\"consulta-parquet\").getOrCreate()\n",
    "\n",
    "# Ruta al archivo Parquet\n",
    "parquet_path = \"dbfs:/FileStore/shared_uploads/alejodiezgomez@gmail.com/raw_flight_data_snappy-1.parquet\"\n",
    "\n",
    "# Lee el archivo Parquet y crea un DataFrame\n",
    "df = spark.read.parquet(parquet_path)\n",
    "\n",
    "# Filtra los vuelos demorados (ArrDelay > 25)\n",
    "vuelos_demorados = df.filter(col(\"ArrDelay\") > 25)\n",
    "\n",
    "# Cuenta la cantidad de vuelos demorados\n",
    "cantidad_vuelos_demorados = vuelos_demorados.count()\n",
    "\n",
    "# Imprime la cantidad de vuelos demorados\n",
    "print(\"Cantidad de vuelos demorados:\", cantidad_vuelos_demorados)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Untitled Notebook 2023-11-17 10:47:36",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
